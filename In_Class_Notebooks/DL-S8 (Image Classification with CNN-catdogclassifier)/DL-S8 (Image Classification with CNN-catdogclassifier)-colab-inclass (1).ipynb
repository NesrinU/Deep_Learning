{"cells":[{"cell_type":"markdown","metadata":{"id":"QCk7gQaaH3Hy"},"source":["<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"Rossum\"></p>"]},{"cell_type":"markdown","metadata":{"id":"KAckTrSGGTaZ"},"source":["<h1 style=\"text-align: center;\">Deep Learning<br><br>Session - 8<br><br>Image Classification with CNN<br><br>Cat-Dog Classification Project Solution<br><h1>"]},{"cell_type":"markdown","metadata":{"id":"lRyehzQ_GTaa"},"source":["# Dataset Info"]},{"cell_type":"markdown","metadata":{"id":"mFEz-C4FGTab"},"source":["The Dogs vs. Cats dataset is a common computer vision dataset in which pictures are classified as either including a dog or a cat.\n","\n","After the dataset is well studied, it can be used to understand and practice how to design, evaluate, and apply convolutional neural networks for image classification.\n","\n","You will build a classifier with images and try to detect dogs versus cats using CNN.\n","\n","Train set includes 12500 cat-5026 dog images, validation set includes 1219 cat-1071 dog images and test set incgludes 6897 cat and dogs images together. "]},{"cell_type":"markdown","metadata":{"id":"IYElQKIJdpXF"},"source":["# Import Libraries and Export Images from Zip_File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YcSALh_Tqpg"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.image import imread\n","\n","#import warnings\n","#warnings.filterwarnings(\"ignore\")\n","#warnings.warn(\"this will not show\")\n","\n","plt.rcParams[\"figure.figsize\"] = (10,6)\n","\n","sns.set_style(\"whitegrid\")\n","pd.set_option('display.float_format', lambda x: '%.3f' % x)\n","\n","# Set it None to display all rows in the dataframe\n","# pd.set_option('display.max_rows', None)\n","\n","# Set it to None to display all columns in the dataframe\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYdjEdSDGTad"},"outputs":[],"source":["import tensorflow as tf\n","\n","tf.config.list_physical_devices(\"GPU\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxjOB4MPGTae"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-fj5zJdGTaf"},"outputs":[],"source":["import zipfile\n","\n","# Unzip the file\n","zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/DS/Instructor Colab Notebooks/data/cat_dog_data.zip\", \"r\")\n","zip_ref.extractall()\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"IGPobqAnGTaf"},"source":["# Recognizing and Understanding Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUu4M0PXGTag"},"outputs":[],"source":["my_data_dir = 'data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkLkKyt3GTah"},"outputs":[],"source":["os.listdir(my_data_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-EEJyGmGTai"},"outputs":[],"source":["val_path = my_data_dir+'/validation/'\n","train_path = my_data_dir+'/train/'\n","test_path = my_data_dir+'/test/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1do4HJ-oGTai"},"outputs":[],"source":["os.listdir(train_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJalBhbtGTaj"},"outputs":[],"source":["os.listdir(val_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9iH0yNjGTaj"},"outputs":[],"source":["os.listdir(test_path)[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXkZKI0wGTaj"},"outputs":[],"source":["import pathlib\n","\n","data_dir = pathlib.Path(train_path) # turn our training path into a Python path\n","class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories\n","print(class_names)"]},{"cell_type":"markdown","metadata":{"id":"OT8Ui7chGTak"},"source":["**Let's check how many images there are.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdUSzgmWGTak"},"outputs":[],"source":["len(os.listdir(train_path+'cat')), len(os.listdir(train_path+'dog'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6JSrth0GTak"},"outputs":[],"source":["len(os.listdir(val_path+'cat')), len(os.listdir(val_path+'dog'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVd_q18RGTal"},"outputs":[],"source":["# Walk through cell_images directory and list number of files\n","for dirpath, dirnames, filenames in os.walk(my_data_dir):\n","  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"]},{"cell_type":"markdown","metadata":{"id":"ZFgpDr_5GTal"},"source":["**Let's take an example images from both train-cat and train-dog folders to observe process** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJqUk7nfGTal"},"outputs":[],"source":["os.listdir(train_path+'cat')[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Inggk-93GTam"},"outputs":[],"source":["os.listdir(train_path+'dog')[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rW_-vIYSGTam"},"outputs":[],"source":["path1=train_path+'cat'+'/cat.4593.jpg'\n","path2=train_path+'dog'+'/dog.10086.jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqlAA32BGTam"},"outputs":[],"source":["cat_img=imread(path1)\n","dog_img=imread(path2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Lzkqv0cGTan"},"outputs":[],"source":["plt.imshow(cat_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQqGOzNNGTan"},"outputs":[],"source":["plt.imshow(dog_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bt_12PAfGTan"},"outputs":[],"source":["# View an image\n","import random\n","\n","def view_random_image(target_dir, target_class):\n","  # Setup target directory (we'll view images from here)\n","  target_folder = target_dir+target_class\n","\n","  # Get a random image path\n","  random_image = random.sample(os.listdir(target_folder), 1)\n","\n","  # Read in the image and plot it using matplotlib\n","  img = imread(target_folder + \"/\" + random_image[0])\n","  plt.imshow(img)\n","  plt.title(target_class)\n","  plt.axis(\"off\");\n","\n","  print(f\"Image shape: {img.shape}\") # show the shape of the image\n","\n","  return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vQBLce9GTao"},"outputs":[],"source":["# View a random image from the training dataset\n","img = view_random_image(target_dir=train_path,\n","                        target_class=\"cat\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_eToniFGTao"},"outputs":[],"source":["# View a random image from the training dataset\n","img = view_random_image(target_dir=train_path,\n","                        target_class=\"dog\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MssOeDFGTao"},"outputs":[],"source":["# View a random image from the training dataset\n","import random\n","img = view_random_image(target_dir=train_path,\n","                        target_class=random.choice(class_names)) # get a random class name"]},{"cell_type":"markdown","metadata":{"id":"DY_IpdOLGTao"},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"9OHj01mvGTao"},"source":["## Defining Input Shape"]},{"cell_type":"markdown","metadata":{"id":"h1jQAQIaGTap"},"source":["**Let's decide on the final dimension of these images.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_ZDAbpgGTap"},"outputs":[],"source":["cat_img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sO_rIlFuGTap"},"outputs":[],"source":["dog_img.shape"]},{"cell_type":"raw","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1649348586960,"user":{"displayName":"Steve Wallace","userId":"06741533186265949524"},"user_tz":-180},"id":"7yeTCZUHGTaq","outputId":"fa8c5fbb-a998-465a-d884-9a5fef3ba1fc"},"source":["\"\"\"\n","x = []\n","y = []\n","for image in os.listdir(train_path+'/cat'):\n","\n","    img = imread(train_path+'/cat/'+image)\n","    x.append(d1)\n","    y.append(d2)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSCT_CNJGTaq"},"outputs":[],"source":["x = [imread(train_path+'cat/'+image).shape[0] for image in os.listdir(train_path+'cat')]\n","y = [imread(train_path+'cat/'+image).shape[1] for image in os.listdir(train_path+'cat')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PaOPngkXGTaq"},"outputs":[],"source":["sns.scatterplot(x,y);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uC3Rw0yuGTaq"},"outputs":[],"source":["np.mean(x), np.median(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMF6drSYGTaq"},"outputs":[],"source":["np.mean(y), np.median(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vetcEFemGTar"},"outputs":[],"source":["image_shape = (128,128,3)"]},{"cell_type":"markdown","metadata":{"id":"khJGGhFYGTar"},"source":["## Scalling"]},{"cell_type":"markdown","metadata":{"id":"E7Q2YN1vGTar"},"source":["**Let's check the images if they are needed to be scaled or not**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ny_kIxJgGTar","scrolled":true},"outputs":[],"source":["cat_img.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbETrs6ZGTar"},"outputs":[],"source":["cat_img.min()"]},{"cell_type":"markdown","metadata":{"id":"xmH3-HtoGTar"},"source":["As we see above, images need to be scaled"]},{"cell_type":"markdown","metadata":{"id":"sKZ32NunGTas"},"source":["## Image Data Generator"]},{"cell_type":"markdown","metadata":{"id":"QlO_Ei6NGTas"},"source":["**Image Manipulation**\n","\n","We can use the ImageDataGenerator to manipulate the images with rotation, resizing, and scaling so the model becomes more robust to different images that our data set doesn't have. ImageDataGenerator does the followings.\n","\n","* Accepts a batch of images used for training.\n","* Applies a series of random transformations to each image in the batch.\n","* Replaces the original batch with randomly transformed batch.\n","* Training the CNN on this randomly transformed batch.\n","\n","The goal of applying data augmentation is to have a more generalized model.\n","\n","Data augmentation is a way to try and prevent a model overfitting. If your model is overfiting (e.g. the validation loss keeps increasing), you may want to try using data augmentation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKpivuOQGTas"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sGRnBAGGTas"},"outputs":[],"source":["# help(ImageDataGenerator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TbBCTK5GTas"},"outputs":[],"source":["image_gen = ImageDataGenerator(rotation_range=15, # rotate the image 15 degrees\n","                               width_shift_range=0.10, # Shift the pic width by a max of 10%\n","                               height_shift_range=0.10, # Shift the pic height by a max of 10%\n","                               rescale=1/255, # Rescale the image by normalzing it.\n","                               shear_range=0.1, # Shear means cutting away part of the image (max 10%)\n","                               zoom_range=0.1, # Zoom in by 10% max\n","                               horizontal_flip=True, # Allow horizontal flipping\n","                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n","                              )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2pV1kxHGTat"},"outputs":[],"source":["plt.imshow(dog_img);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmPba4hFGTat"},"outputs":[],"source":["plt.imshow(image_gen.random_transform(dog_img));"]},{"cell_type":"markdown","metadata":{"id":"ifK_cFBVGTat"},"source":["### Taking the path to a directory & Generating batches of augmented data\n","\n","flow_from_directory function works with images organized in sub-directories. Your directories should include only one class of images, so one folder per class of images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pslwwXmGTat"},"outputs":[],"source":["#help(image_gen.flow_from_directory)\n","#Takes the path to a directory & generates batches of augmented data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_aC09CAGTat"},"outputs":[],"source":["image_gen.flow_from_directory(train_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qoYmJFZhGTau"},"outputs":[],"source":["image_gen.flow_from_directory(val_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Zs3ZFvpGTau"},"outputs":[],"source":["image_gen.flow_from_directory(test_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-BZAY_pGTau"},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFL7yI3AGTau"},"outputs":[],"source":["train_image_gen = image_gen.flow_from_directory(directory=train_path,\n","                                                target_size=image_shape[:2],\n","                                                color_mode='rgb',\n","                                                batch_size=batch_size,\n","                                                class_mode='binary',\n","                                                shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAAwdlm7GTau"},"outputs":[],"source":["val_image_gen = image_gen.flow_from_directory(directory=val_path,\n","                                              target_size=image_shape[:2],\n","                                              color_mode='rgb',\n","                                              batch_size=batch_size,\n","                                              class_mode='binary',\n","                                              shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m4RAAZeGTav","scrolled":true},"outputs":[],"source":["train_image_gen.class_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0nVKjRbGTav"},"outputs":[],"source":["train_image_gen[0][0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKDGco0MGTav"},"outputs":[],"source":["len(train_image_gen), len(val_image_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwRh-93-GTav"},"outputs":[],"source":["len(train_image_gen)*batch_size, len(val_image_gen)*batch_size "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KqfozfFGTav"},"outputs":[],"source":["# Get a sample of the training data batch \n","images, labels = train_image_gen.next() # get the 'next' batch of images/labels\n","len(images), len(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8wOq5JyGTaw"},"outputs":[],"source":["# Get a sample of the testing data batch \n","images, labels = val_image_gen.next() # get the 'next' batch of images/labels\n","len(images), len(labels)"]},{"cell_type":"markdown","metadata":{"id":"zpXcU5JVGTaw"},"source":["# Modelling-1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFguNwvCGTaw"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOcNinE9GTaw"},"outputs":[],"source":["model1 = Sequential()\n","\n","model1.add(Conv2D(filters=16, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\n","model1.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n","model1.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model1.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n","model1.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model1.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n","model1.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model1.add(Flatten())\n","\n","model1.add(Dense(128))\n","model1.add(Activation('relu'))\n","\n","model1.add(Dense(64))\n","model1.add(Activation('relu'))\n","\n","model1.add(Dense(1))\n","model1.add(Activation('sigmoid'))\n","\n","model1.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjqNpfaHGTaw"},"outputs":[],"source":["model1.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zEFLHlgGTaw"},"outputs":[],"source":["early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAo4WFhvGTax"},"outputs":[],"source":["model1.fit(train_image_gen,\n","          epochs=10,\n","          steps_per_epoch=len(train_image_gen),\n","          validation_data=val_image_gen,\n","          validation_steps=len(val_image_gen),\n","          callbacks=[early_stop])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HzM5r8oGTax"},"outputs":[],"source":["summary = pd.DataFrame(model1.history.history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XMGXxNPoGTay"},"outputs":[],"source":["summary[[\"loss\", \"val_loss\"]].plot();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYHRRrJbGTay"},"outputs":[],"source":["summary[[\"accuracy\", \"val_accuracy\"]].plot();"]},{"cell_type":"markdown","metadata":{"id":"gN9yUrUfGTa0"},"source":["# Modelling-2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBx7tpeWdpXf"},"outputs":[],"source":["model2 = Sequential()\n","\n","model2.add(Conv2D(filters=16, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\n","model2.add(BatchNormalization())\n","model2.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model2.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n","model2.add(BatchNormalization())\n","model2.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model2.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n","model2.add(BatchNormalization())\n","model2.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model2.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n","model2.add(BatchNormalization())\n","model2.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model2.add(Flatten())\n","\n","model2.add(Dense(128))\n","model2.add(Activation('relu'))\n","\n","model2.add(Dense(128))\n","model2.add(Activation('relu'))\n","\n","model2.add(Dense(1))\n","model2.add(Activation('sigmoid'))\n","\n","model2.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtwlExQoGTa0"},"outputs":[],"source":["model2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3Eu2ycVGTa0"},"outputs":[],"source":["early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GZAlvgVGTa0"},"outputs":[],"source":["model2.fit(train_image_gen,\n","          epochs=15,\n","          steps_per_epoch=len(train_image_gen),\n","          validation_data=val_image_gen,\n","          validation_steps=len(val_image_gen),\n","          callbacks=[early_stop])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BClXgRrPGTa1"},"outputs":[],"source":["summary = pd.DataFrame(model2.history.history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tih-6KSoGTa1"},"outputs":[],"source":["summary[[\"loss\", \"val_loss\"]].plot();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-OAyQKfGTa1"},"outputs":[],"source":["summary[[\"accuracy\", \"val_accuracy\"]].plot();"]},{"cell_type":"markdown","metadata":{"id":"QTmc3VAYyo7F"},"source":["# Modelling-3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eNcMzSgUyo7F"},"outputs":[],"source":["model3 = Sequential()\n","\n","model3.add(Conv2D(filters=16, kernel_size=(3,3), padding='same', input_shape=image_shape, activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model3.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model3.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model3.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n","model3.add(BatchNormalization())\n","model3.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model3.add(Flatten())\n","\n","model3.add(Dense(128))\n","model3.add(Activation('relu'))\n","\n","model3.add(Dense(128))\n","model3.add(Activation('relu'))\n","\n","model3.add(Dense(1))\n","model3.add(Activation('sigmoid'))\n","\n","model3.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rl8oS5rkyo7G"},"outputs":[],"source":["model3.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrPv9TrFyo7G"},"outputs":[],"source":["early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYXRxDztyo7G"},"outputs":[],"source":["model3.fit(train_image_gen,\n","          epochs=20,\n","          steps_per_epoch=len(train_image_gen),\n","          validation_data=val_image_gen,\n","          validation_steps=len(val_image_gen),\n","          callbacks=[early_stop])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtKDzg-Tyo7G"},"outputs":[],"source":["summary = pd.DataFrame(model3.history.history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XBpjTu7yo7G"},"outputs":[],"source":["summary[[\"loss\", \"val_loss\"]].plot();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wttZqrayyo7H"},"outputs":[],"source":["summary[[\"accuracy\", \"val_accuracy\"]].plot();"]},{"cell_type":"code","source":[""],"metadata":{"id":"xueQyuZUyo7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FhT4Mm6_yiOS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dwlYjM6GTa1"},"source":["# Evaluation on Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6u9z-nSyGTay"},"outputs":[],"source":["from sklearn.metrics import classification_report, confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGvHNDgOGTay"},"outputs":[],"source":["score = model2.evaluate(val_image_gen)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kBp6TlUGTaz"},"outputs":[],"source":["pred_prob = model2.predict(val_image_gen)\n","pred_prob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bHXdbFtGTaz"},"outputs":[],"source":["y_pred = pred_prob > 0.5\n","y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vY1Ia5JGTaz"},"outputs":[],"source":["y_test = val_image_gen.classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rTEKTEQGTaz"},"outputs":[],"source":["print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qluGVvDmGTaz","scrolled":true},"outputs":[],"source":["confusion_matrix(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bTHuc_7GTa3"},"outputs":[],"source":["model2.save('cat_dog_detector.h5')"]},{"cell_type":"markdown","metadata":{"id":"4TWrdbLfGTa3"},"source":["# Prediction-1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEHiZDqYGTa3"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing import image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9AzhacaGTa3"},"outputs":[],"source":["model=load_model('cat_dog_detector.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_U6UvjwMjYw"},"outputs":[],"source":["img_path = \"/content/drive/MyDrive/DS/Instructor Colab Notebooks/data/cat_test.jpg\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxqApltxGTa4"},"outputs":[],"source":["img=image.load_img(img_path)\n","img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSqPSGlNGTa4"},"outputs":[],"source":["img = np.array(img)"]},{"cell_type":"code","source":["img.shape"],"metadata":{"id":"nPRE6ZTyBuRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esXf-VBXGTa4"},"outputs":[],"source":["resized_img = image.smart_resize(img, (128, 128)) # img has to be numpy array \n","resized_img.shape\n","\n","#resized_img = img.resize((128, 128)) # img has to be image format (like jpg, png etc)\n","#resized_img"]},{"cell_type":"code","source":["resized_img.max()"],"metadata":{"id":"zB9V2GiQCC38"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKwtCenSGTa5"},"outputs":[],"source":["resized_img = resized_img / 255"]},{"cell_type":"code","source":["plt.imshow(resized_img)"],"metadata":{"id":"TWGvf6qGB0Oc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2HhV_8PGTa5"},"outputs":[],"source":["resized_img=np.expand_dims(resized_img, axis=0)\n","resized_img.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GTJlN7mGTa5"},"outputs":[],"source":["model.predict(resized_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNlgi_9yGTa5"},"outputs":[],"source":["val_image_gen.class_indices"]},{"cell_type":"markdown","metadata":{"id":"2PQmj21ydpXl"},"source":["# Prediction-2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fppLqq04dpXl"},"outputs":[],"source":["random_image = random.sample(os.listdir(test_path), 1)\n","random_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jycEBehTdpXl"},"outputs":[],"source":["image_path = test_path + \"/\" + random_image[0]\n","image_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EbOmTyC3dpXl"},"outputs":[],"source":["my_image = image.load_img(image_path, target_size=image_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzcNoPR7dpXl"},"outputs":[],"source":["my_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRj3cH8IdpXm"},"outputs":[],"source":["type(my_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjjuy0kTdpXm"},"outputs":[],"source":["#my_image = np.array(my_image)\n","my_image = image.img_to_array(my_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5KIi5xVdpXm"},"outputs":[],"source":["my_image.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6bqUOc_dpXm"},"outputs":[],"source":["my_image = my_image / 255"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lB-7tWmUdpXm"},"outputs":[],"source":["my_image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHI7E7LhdpXm"},"outputs":[],"source":["my_image = np.expand_dims(my_image, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RyxmQNAdpXn"},"outputs":[],"source":["my_image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQ1TDQMmdpXn"},"outputs":[],"source":["model.predict(my_image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLelbX8TdpXn"},"outputs":[],"source":["train_image_gen.class_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAGHFHn5dpXn"},"outputs":[],"source":["def pred_and_plot(model, img_size):\n","    \"\"\"\n","    Imports an image located at filename, makes a prediction on it with\n","    a trained model and plots the image with the predicted class as the title.\n","    \"\"\"\n","    # Import the target image and preprocess it\n","    random_image = random.sample(os.listdir(test_path), 1)\n","    img_path = test_path + \"/\" + random_image[0]\n","    img = image.load_img(img_path, target_size=img_size)\n","    img = np.array(img)\n","    if img.max() > 1:\n","        img = img/255\n","  \n","    # Make a prediction\n","    pred = model.predict(np.expand_dims(img, axis=0))\n","    print(\"prediction_probability: \", pred.max())\n","\n","    # Get the predicted class\n","    if len(pred[0]) > 1: # check for multi-class\n","        pred_class = class_names[pred.argmax()] # if more than one output, take the max\n","    else:\n","        pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n","\n","    # Plot the image and predicted class\n","    plt.imshow(img)\n","    plt.title(f\"Prediction: {pred_class}\")\n","    plt.axis(False);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Oe8TV06GTa6"},"outputs":[],"source":["pred_and_plot(model, image_shape[:2])"]},{"cell_type":"markdown","metadata":{"id":"d7rpqC16GTa6"},"source":["<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"Rossum\"></p>"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["5hU26rwQGTay"],"name":"DL-S8 (Image Classification with CNN-catdogclassifier)-colab-inclass.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}